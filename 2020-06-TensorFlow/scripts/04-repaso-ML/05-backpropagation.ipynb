{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propagación hacia atrás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\lncostam\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "\n",
    "# Para que funcione con la version 2 instalada\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de Regresión\n",
    "- $X \\sim N(1.0, 0.1)$\n",
    "- $y = Ax, A = 10$\n",
    "- target = 10\n",
    "- $L2$ función de pérdidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar el radianti desendiente, vamos a tener 10 como predictor,\n",
    "Los datos tienen una media de uno,\n",
    "Raio de aprendizaje, parametro que ayuda a la convergencia del algoritmo, vamos a usar radiante desendiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = np.random.normal(loc=1, scale=0.1, size=200) # valores\n",
    "y_vals = np.repeat(10., 200) # nuestro valor real de la prediccion, a lo que debo llegar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = tf.placeholder(shape = [1], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape = [1], dtype=tf.float32)\n",
    "A = tf.Variable(tf.random_normal(shape=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pred = tf.multiply(A, x_data) # A * x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.square(my_pred - y_target) # real - prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_optim = tf.train.GradientDescentOptimizer(learning_rate=0.025) # Mas pequeño, mas tiempo estara para converger al resultado\n",
    "# learning_rate -> el porcentaje que hago de cambio de direccion, si le pongo 0.9 me paso al otro lado de una\n",
    "\n",
    "# El gradiente desendiente minimiza la funcion de perdida\n",
    "train_step = my_optim.minimize(loss) # Minimizamos la funcion de perdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso #20 A = [6.381801] Loss: [22.501003]\n",
      "Paso #40 A = [8.81249] Loss: [4.315429]\n",
      "Paso #60 A = [9.548436] Loss: [1.178583]\n",
      "Paso #80 A = [9.532453] Loss: [0.04634365]\n",
      "Paso #100 A = [9.710582] Loss: [0.15236297]\n",
      "Paso #120 A = [9.931706] Loss: [0.25255087]\n",
      "Paso #140 A = [10.069696] Loss: [0.07327254]\n",
      "Paso #160 A = [10.179722] Loss: [3.8393948]\n",
      "Paso #180 A = [10.078401] Loss: [0.34910807]\n",
      "Paso #200 A = [10.023293] Loss: [0.2656489]\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    rand_index = np.random.choice(200) #Elegimos aleatoriamente uno de ellos\n",
    "    rand_x = [x_vals[rand_index]]\n",
    "    rand_y = [y_vals[rand_index]]\n",
    "    session.run(train_step, feed_dict={x_data : rand_x, y_target : rand_y})\n",
    "    if (i+1)%20==0:\n",
    "        print(\"Paso #\"+str(i+1)+\" A = \"+str(session.run(A))  +\n",
    "              \" Loss: \"+str(session.run(loss, feed_dict={x_data:rand_x, y_target: rand_y})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de clasificación binaria\n",
    "- $X_1\\sim N(-2, 1), X_2 \\sim N(2,1)$\n",
    "- $target(X_1) = 0, target(X_2) = 1$\n",
    "- sigmoid(x+A) = $\\frac{1}{1+e^{-(x+A)}}$\n",
    "- Determinar el valor de $A$\n",
    "- Teóricamente $A\\simeq \\frac{m_1+m_2}{2}, m_1 = -2, m_2 = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph() # Recetear todos los valores ( el grafo )\n",
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = np.concatenate((np.random.normal(-2,1,100), np.random.normal(2,1,100))) # valores a trabajar\n",
    "y_vals = np.concatenate((np.repeat(0., 100), np.repeat(1., 100))) # Valores a los que debo llegar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = tf.placeholder(shape = [1], dtype = tf.float32) # valores a trabajar\n",
    "y_target = tf.placeholder(shape = [1], dtype = tf.float32) # Valores a los que debo llegar\n",
    "A = tf.Variable(tf.random_normal(mean = 10, shape=[1])) # Lo inicializo a 10 que esta muy lejos del 0 que es el valor real\n",
    "                                                        # es para demostrar como funciona el algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prediction = tf.add(x_data, A) # x + A -> valor del exponente de la sigmoide en este caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prediction_expanded = tf.expand_dims(my_prediction, 0) # agrego una dimension mas\n",
    "y_target_expanded = tf.expand_dims(y_target,0 ) # agrego una dimension mas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.168033]\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "session.run(init)\n",
    "print(session.run(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Es un problema de clasificacion, por eso debemos utilizar un algoritmo del mismo (clasificacion)\n",
    "xentr = tf.nn.sigmoid_cross_entropy_with_logits(logits=my_prediction_expanded, labels=y_target_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos el Gradient Descent\n",
    "# Si tengo varios minimos deberia intentar utilizar otra o una variante de GD\n",
    "my_optim = tf.train.GradientDescentOptimizer(learning_rate=0.04)\n",
    "train_step = my_optim.minimize(xentr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso #100, A = [-0.21891217], Loss = [[0.23432809]]\n",
      "Paso #200, A = [-0.2237996], Loss = [[0.05712586]]\n",
      "Paso #300, A = [-0.293709], Loss = [[0.00596012]]\n",
      "Paso #400, A = [-0.24515887], Loss = [[0.05189894]]\n",
      "Paso #500, A = [-0.14302775], Loss = [[0.09144955]]\n",
      "Paso #600, A = [-0.06524014], Loss = [[0.49811634]]\n",
      "Paso #700, A = [-0.11472588], Loss = [[0.03044535]]\n",
      "Paso #800, A = [-0.15946701], Loss = [[0.2128054]]\n",
      "Paso #900, A = [-0.20810771], Loss = [[0.13491595]]\n",
      "Paso #1000, A = [-0.16033426], Loss = [[0.05968192]]\n",
      "Paso #1100, A = [-0.24976729], Loss = [[0.26082292]]\n",
      "Paso #1200, A = [-0.0955787], Loss = [[0.07911838]]\n",
      "Paso #1300, A = [-0.04912282], Loss = [[0.18254389]]\n",
      "Paso #1400, A = [0.05932139], Loss = [[0.2500807]]\n",
      "Paso #1500, A = [0.00299393], Loss = [[0.051944]]\n",
      "Paso #1600, A = [0.04730058], Loss = [[0.06326841]]\n",
      "Paso #1700, A = [0.05241134], Loss = [[0.04198264]]\n",
      "Paso #1800, A = [-0.01110295], Loss = [[0.20893495]]\n",
      "Paso #1900, A = [-0.04025513], Loss = [[0.3258866]]\n",
      "Paso #2000, A = [-0.17485258], Loss = [[0.10223212]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(2000):\n",
    "    rand_idx = np.random.choice(200) # Selecciono un numero aleatorio\n",
    "    rand_x = [x_vals[rand_idx]]\n",
    "    rand_y = [y_vals[rand_idx]]\n",
    "    session.run(train_step, feed_dict={x_data : rand_x, y_target : rand_y})\n",
    "    if (i+1)%100==0:\n",
    "        print(\"Paso #\"+str(i+1)+\", A = \"+str(session.run(A))+\", Loss = \"+\n",
    "             str(session.run(xentr, feed_dict={x_data : rand_x, y_target: rand_y})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
